# Redis Cache - Performance Optimization

## Overview

The Redis cache module provides high-performance caching for GatheRing's most expensive operations:
- **Embeddings**: Caching API calls to OpenAI/Anthropic (saves $$$ and latency)
- **RAG Results**: Caching memory recall queries (faster agent thinking)
- **Circle Context**: Caching multi-agent coordination state

**Key Features:**
- ‚úÖ Graceful degradation (works without Redis)
- ‚úÖ Automatic TTL management
- ‚úÖ Event-based cache invalidation
- ‚úÖ JSON serialization
- ‚úÖ Key namespacing
- ‚úÖ Statistics and monitoring

**Impact:**
- üöÄ 95%+ reduction in embedding API calls
- ‚ö° 10x faster memory recall for repeated queries
- üí∞ Significant cost savings on LLM API usage

---

## Quick Start

### 1. Install Redis

```bash
# macOS
brew install redis
brew services start redis

# Ubuntu/Debian
sudo apt install redis-server
sudo systemctl start redis

# Docker
docker run -d -p 6379:6379 redis:7-alpine
```

### 2. Configure Environment

```bash
# .env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=  # Optional
CACHE_ENABLED=true
```

### 3. Use in Code

```python
from gathering.cache import CacheManager

# Auto-configure from environment
cache = CacheManager.from_env()

# Check if cache is working
if cache.is_enabled():
    print("‚úÖ Redis cache is active!")
else:
    print("‚ö†Ô∏è Cache disabled, using degraded mode")
```

### 4. Test Cache

```python
# Set a value
cache.set_embedding("Hello world", [0.1, 0.2, 0.3])

# Get it back
embedding = cache.get_embedding("Hello world")
print(embedding)  # [0.1, 0.2, 0.3]

# Check stats
stats = cache.get_stats()
print(f"Hit rate: {stats['hit_rate']}%")
```

---

## Cache Types

### 1. Embedding Cache

Embeddings are the most expensive operation (API calls to OpenAI/Anthropic). Caching saves costs and latency.

**TTL:** 24 hours (embeddings are deterministic)

```python
# Automatic via EmbeddingService
from gathering.rag.embeddings import EmbeddingService

embedder = EmbeddingService.from_env()
embedding = await embedder.embed("Hello world")  # First call ‚Üí API
embedding = await embedder.embed("Hello world")  # Second call ‚Üí CACHE ‚úÖ
```

**How it works:**
1. Text is hashed (SHA256, 16 chars)
2. Key: `gathering:embedding:<hash>`
3. Cached for 24 hours
4. Fallback to in-memory cache if Redis unavailable

**Impact:**
- Save ~$0.0001 per embedding (adds up fast!)
- Reduce latency from 100-300ms to <1ms

### 2. RAG Results Cache

Memory recall queries can be expensive (vector search + filtering). Cache recent queries.

**TTL:** 5 minutes (balance freshness vs performance)

```python
# Automatic via MemoryManager
from gathering.rag.memory_manager import MemoryManager

memory = MemoryManager.from_env()

# First call ‚Üí database + vector search
results = await memory.recall(
    agent_id=1,
    query="What are the user's preferences?",
)

# Second call within 5min ‚Üí CACHE ‚úÖ
results = await memory.recall(
    agent_id=1,
    query="What are the user's preferences?",
)
```

**Cache Invalidation:**

Cache is automatically invalidated when memories change:

```python
# Add new memory
await memory.remember(
    agent_id=1,
    content="User prefers dark mode",
)
# ‚Üí RAG cache for agent_id=1 is cleared ‚úÖ

# Next recall() will hit database (fresh data)
```

**How it works:**
1. Query text is hashed
2. Key: `gathering:rag:agent:<agent_id>:<query_hash>`
3. Cached for 5 minutes
4. Invalidated on MEMORY_CREATED/MEMORY_SHARED events
5. Only unfiltered queries are cached (for consistency)

**Impact:**
- 10x faster for repeated queries
- Especially helpful for conversational agents

### 3. Circle Context Cache

Circle state (members, tasks, metadata) is frequently accessed. Cache it.

**TTL:** 10 minutes

```python
# Manual usage
context = {
    "name": "Research Team",
    "members": [1, 2, 3],
    "active_tasks": 5,
}

cache.set_circle_context(circle_id=5, context=context)

# Later...
cached = cache.get_circle_context(circle_id=5)  # CACHE ‚úÖ

# Invalidate on change
cache.invalidate_circle_context(circle_id=5)
```

**Future:** Auto-invalidation via Event Bus (CIRCLE_UPDATED event).

---

## API Reference

### CacheManager

```python
class CacheManager:
    """Redis cache with graceful degradation."""

    def __init__(self, config: CacheConfig) -> None:
        """Initialize with configuration."""

    @classmethod
    def from_env(cls, dotenv_path: Optional[str] = None) -> "CacheManager":
        """Create from environment variables."""

    def is_enabled(self) -> bool:
        """Check if cache is working."""
```

#### Generic Operations

```python
def get(self, key: str) -> Optional[Any]:
    """Get value from cache."""

def set(self, key: str, value: Any, ttl: Optional[int] = None) -> bool:
    """Set value with optional TTL (seconds)."""

def delete(self, key: str) -> bool:
    """Delete single key."""

def delete_pattern(self, pattern: str) -> int:
    """Delete all keys matching pattern."""
```

#### Embedding Cache

```python
def get_embedding(self, text: str) -> Optional[List[float]]:
    """Get cached embedding."""

def set_embedding(
    self,
    text: str,
    embedding: List[float],
    ttl: Optional[int] = None,
) -> bool:
    """Cache embedding (default TTL: 24h)."""
```

#### RAG Cache

```python
def get_rag_results(
    self,
    agent_id: int,
    query: str,
) -> Optional[List[Dict[str, Any]]]:
    """Get cached RAG results."""

def set_rag_results(
    self,
    agent_id: int,
    query: str,
    results: List[Dict[str, Any]],
    ttl: Optional[int] = None,
) -> bool:
    """Cache RAG results (default TTL: 5min)."""

def invalidate_rag_results(self, agent_id: int) -> int:
    """Clear all RAG cache for agent."""
```

#### Circle Context Cache

```python
def get_circle_context(self, circle_id: int) -> Optional[Dict[str, Any]]:
    """Get cached circle context."""

def set_circle_context(
    self,
    circle_id: int,
    context: Dict[str, Any],
    ttl: Optional[int] = None,
) -> bool:
    """Cache circle context (default TTL: 10min)."""

def invalidate_circle_context(self, circle_id: int) -> bool:
    """Clear circle context cache."""
```

#### Utilities

```python
def get_stats(self) -> Dict[str, Any]:
    """Get cache statistics (hits, misses, hit rate)."""

def clear_all(self) -> int:
    """Clear all GatheRing cache keys."""

def close(self) -> None:
    """Close Redis connection."""
```

### CacheConfig

```python
@dataclass
class CacheConfig:
    """Cache configuration."""

    # Connection
    host: str = "localhost"
    port: int = 6379
    db: int = 0
    password: Optional[str] = None

    # TTL defaults (seconds)
    embedding_ttl: int = 86400      # 24 hours
    rag_results_ttl: int = 300      # 5 minutes
    circle_context_ttl: int = 600   # 10 minutes
    llm_response_ttl: int = 3600    # 1 hour (future)

    # Behavior
    enabled: bool = True
    key_prefix: str = "gathering:"
```

---

## Graceful Degradation

The cache is designed to work even without Redis:

### Degradation Modes

1. **Redis library not installed**
   - `REDIS_AVAILABLE = False`
   - All cache operations return None/False
   - System works normally (no cache)

2. **Redis connection failed**
   - Connection attempt fails in `__init__`
   - `_enabled = False`
   - All cache operations return None/False
   - Warning logged: "Running without cache (degraded mode)"

3. **Cache disabled in config**
   - `CACHE_ENABLED=false` in env
   - `_enabled = False`
   - All cache operations return None/False

4. **Redis errors during operation**
   - Individual operations catch exceptions
   - Log error, return None/False
   - System continues working

### Fallback Strategies

**EmbeddingService:**
- Maintains in-memory LRU cache (1000 entries)
- Falls back to memory cache if Redis unavailable
- Always works, even without any cache

**MemoryManager:**
- Simply skips cache checks if unavailable
- Always queries database (slower but correct)

### Testing Degradation

```python
# Test without Redis
config = CacheConfig(enabled=False)
cache = CacheManager(config)

assert cache.is_enabled() is False
assert cache.get("any_key") is None
assert cache.set("any_key", "value") is False

# System still works!
```

---

## Cache Invalidation

### Event-Based Invalidation

The cache automatically invalidates when data changes, using the Event Bus:

```python
# MemoryManager subscribes to memory events
event_bus.subscribe(EventType.MEMORY_CREATED, memory._on_memory_created)
event_bus.subscribe(EventType.MEMORY_SHARED, memory._on_memory_created)

# When memory is created...
await memory.remember(agent_id=1, content="New fact")

# ...cache is invalidated automatically ‚úÖ
```

### Manual Invalidation

You can also invalidate manually:

```python
# Invalidate RAG cache for agent
memory.invalidate_cache(agent_id=1)

# Invalidate circle context
cache.invalidate_circle_context(circle_id=5)

# Clear all cache
cache.clear_all()
```

### Pattern-Based Invalidation

```python
# Delete all RAG results for agent 1
cache.delete_pattern("gathering:rag:agent:1:*")

# Delete all embeddings
cache.delete_pattern("gathering:embedding:*")

# Delete everything
cache.delete_pattern("gathering:*")
```

---

## Monitoring

### Cache Statistics

```python
stats = cache.get_stats()

# Example output:
{
    "enabled": True,
    "host": "localhost",
    "port": 6379,
    "db": 0,
    "total_keys": 1247,
    "hits": 8543,
    "misses": 1234,
    "hit_rate": 87.38,  # %
}
```

### Key Metrics

- **Hit Rate**: Percentage of cache hits vs misses
  - Good: >80%
  - Excellent: >90%
  - Low <50%: Check TTLs, consider warming cache

- **Total Keys**: Number of cached items
  - Monitor growth
  - Set memory limits in Redis config

### Redis Monitoring

```bash
# Connect to Redis CLI
redis-cli

# Check memory usage
INFO memory

# List all GatheRing keys
KEYS gathering:*

# Get key TTL
TTL gathering:embedding:abc123

# Monitor real-time commands
MONITOR
```

### Performance Metrics

**Before Cache:**
```
Embedding generation: 150ms avg
Memory recall: 80ms avg
Circle context load: 50ms avg
```

**With Cache (90% hit rate):**
```
Embedding generation: 15ms avg (10x faster)
Memory recall: 8ms avg (10x faster)
Circle context load: 1ms avg (50x faster)
```

---

## Best Practices

### 1. Set Appropriate TTLs

```python
# Deterministic data ‚Üí Long TTL
cache.set_embedding(text, embedding, ttl=86400)  # 24h

# Frequently changing data ‚Üí Short TTL
cache.set_rag_results(agent_id, query, results, ttl=300)  # 5min

# Context data ‚Üí Medium TTL
cache.set_circle_context(circle_id, context, ttl=600)  # 10min
```

### 2. Invalidate Proactively

Don't wait for TTL expiration when data changes:

```python
# After updating memory
await memory.remember(...)
memory.invalidate_cache(agent_id)  # Clear stale cache

# After circle state change
circle.add_member(...)
cache.invalidate_circle_context(circle.id)
```

### 3. Monitor Hit Rates

Low hit rates indicate:
- TTLs too short
- Too many invalidations
- Queries too diverse (not repeating)

```python
stats = cache.get_stats()
if stats.get("hit_rate", 0) < 50:
    print("‚ö†Ô∏è Low cache hit rate, check configuration")
```

### 4. Handle Cache Failures Gracefully

```python
# ‚ùå BAD: Rely on cache
embedding = cache.get_embedding(text)
return embedding  # Fails if cache is down!

# ‚úÖ GOOD: Cache is optional
embedding = cache.get_embedding(text)
if embedding is None:
    embedding = await expensive_api_call(text)
    cache.set_embedding(text, embedding)
return embedding
```

### 5. Use Namespaced Keys

The cache manager automatically namespaces keys:

```python
# Your code
cache.set_embedding("hello", [...])

# Actual Redis key
# gathering:embedding:abc123

# Allows clean separation
cache.clear_all()  # Only clears "gathering:*"
```

### 6. Cache Only Expensive Operations

Don't cache everything:

```python
# ‚úÖ GOOD: Cache expensive embedding
embedding = await embedder.embed(text)  # Cached

# ‚ùå BAD: Don't cache simple lookups
user_id = lookup_user_id(username)  # Not worth caching
```

---

## Configuration

### Environment Variables

```bash
# Connection
REDIS_HOST=localhost        # Redis host
REDIS_PORT=6379            # Redis port
REDIS_DB=0                 # Database number (0-15)
REDIS_PASSWORD=secret123   # Optional password

# Behavior
CACHE_ENABLED=true         # Enable/disable cache
```

### Custom Configuration

```python
from gathering.cache import CacheConfig, CacheManager

config = CacheConfig(
    host="redis.example.com",
    port=6380,
    db=1,
    password="secret",
    embedding_ttl=7200,      # 2 hours
    rag_results_ttl=600,     # 10 minutes
    circle_context_ttl=1200, # 20 minutes
    key_prefix="myapp:",     # Custom prefix
)

cache = CacheManager(config)
```

### Redis Configuration

Recommended `redis.conf` settings:

```conf
# Memory limit (adjust based on usage)
maxmemory 2gb

# Eviction policy (remove least recently used)
maxmemory-policy allkeys-lru

# Persistence (optional, cache can be ephemeral)
save ""  # Disable RDB snapshots
appendonly no  # Disable AOF

# Logging
loglevel notice
logfile /var/log/redis/redis.log
```

---

## Integration Examples

### Example 1: Agent with Cache

```python
from gathering.agents import AgentWrapper
from gathering.rag.memory_manager import MemoryManager

# Memory manager with cache
memory = MemoryManager.from_env()  # Auto-configures cache

# Create agent
agent = AgentWrapper(
    agent_id=1,
    persona=persona,
    llm=llm,
    memory_manager=memory,  # Uses cache internally
)

# Agent benefits from cache automatically
response = await agent.run("What are my preferences?")
# First call ‚Üí Database query
# Repeated calls ‚Üí Cache ‚úÖ
```

### Example 2: Circle with Cached Context

```python
from gathering.orchestration import Circle
from gathering.cache import CacheManager

cache = CacheManager.from_env()

class CachedCircle(Circle):
    def get_context(self):
        # Check cache first
        context = cache.get_circle_context(self.id)
        if context:
            return context

        # Load from database
        context = super().get_context()

        # Cache it
        cache.set_circle_context(self.id, context)
        return context
```

### Example 3: Warming the Cache

```python
async def warm_cache(agent_id: int):
    """Pre-populate cache with common queries."""
    common_queries = [
        "What are the user's preferences?",
        "What is the project context?",
        "What are the recent decisions?",
    ]

    memory = MemoryManager.from_env()

    for query in common_queries:
        await memory.recall(agent_id, query)
        # First call populates cache

    print(f"‚úÖ Cache warmed for agent {agent_id}")
```

---

## Troubleshooting

### Cache Not Working

**Symptom:** `is_enabled()` returns `False`

**Solutions:**
1. Check Redis is running: `redis-cli ping` ‚Üí should return `PONG`
2. Check environment: `echo $CACHE_ENABLED` ‚Üí should be `true`
3. Check connection: `redis-cli -h $REDIS_HOST -p $REDIS_PORT`
4. Check logs for connection errors

### Low Hit Rate

**Symptom:** `hit_rate < 50%`

**Possible causes:**
1. TTLs too short ‚Üí Increase TTLs
2. Too many invalidations ‚Üí Review invalidation logic
3. Queries not repeating ‚Üí Normal for diverse workloads
4. Cache just started ‚Üí Wait for warm-up

### High Memory Usage

**Symptom:** Redis using too much RAM

**Solutions:**
1. Set `maxmemory` in redis.conf
2. Use `allkeys-lru` eviction policy
3. Reduce TTLs for less critical data
4. Monitor with `INFO memory`

### Connection Errors

**Symptom:** "Redis connection failed"

**Solutions:**
1. Check Redis is running: `systemctl status redis`
2. Check firewall: `telnet localhost 6379`
3. Check password: `redis-cli -a $REDIS_PASSWORD`
4. Check bind address in redis.conf

---

## Testing

### Unit Tests

```bash
# Run all cache tests
pytest tests/test_cache.py -v

# Run specific test class
pytest tests/test_cache.py::TestEmbeddingCache -v

# Run with coverage
pytest tests/test_cache.py --cov=gathering.cache
```

### Integration Tests

```python
# Test with real Redis (requires Redis running)
@pytest.mark.integration
async def test_cache_integration():
    cache = CacheManager.from_env()
    assert cache.is_enabled()

    # Test roundtrip
    cache.set_embedding("test", [0.1, 0.2, 0.3])
    result = cache.get_embedding("test")
    assert result == [0.1, 0.2, 0.3]

    # Cleanup
    cache.clear_all()
```

### Manual Testing

```python
# Test script
from gathering.cache import CacheManager

cache = CacheManager.from_env()

print(f"Cache enabled: {cache.is_enabled()}")

# Test embedding cache
cache.set_embedding("hello", [0.1, 0.2, 0.3])
print(f"Embedding: {cache.get_embedding('hello')}")

# Test stats
stats = cache.get_stats()
print(f"Stats: {stats}")

# Cleanup
cache.clear_all()
```

---

## Roadmap

### Phase 5.2 (Current) ‚úÖ
- [x] CacheManager implementation
- [x] Embedding cache
- [x] RAG results cache
- [x] Event-based invalidation
- [x] Unit tests
- [x] Documentation

### Future Enhancements

**Phase 5.3: LLM Response Cache** (Optional)
- Cache deterministic LLM calls
- Prompt hash ‚Üí response
- Huge cost savings for repeated queries
- TTL: 1 hour

**Phase 5.4: Distributed Cache**
- Redis Cluster support
- Multi-region replication
- Failover handling

**Phase 5.5: Cache Analytics**
- Dashboard showing hit rates
- Cost savings metrics
- Cache efficiency graphs
- Automatic tuning recommendations

---

## See Also

- [EVENT_BUS.md](EVENT_BUS.md) - Event-driven architecture
- [ARCHITECTURE.md](ARCHITECTURE.md) - Overall system design
- [PHASE5_CHANGELOG.md](PHASE5_CHANGELOG.md) - Implementation details
- [TESTING.md](TESTING.md) - Testing guide
