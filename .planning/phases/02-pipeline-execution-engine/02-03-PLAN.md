---
phase: 02-pipeline-execution-engine
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - gathering/orchestration/pipeline/executor.py
  - gathering/orchestration/pipeline/__init__.py
  - gathering/orchestration/__init__.py
  - tests/test_pipeline_validation.py
  - tests/test_pipeline_execution.py
autonomous: true

must_haves:
  truths:
    - "A running pipeline can be cancelled mid-execution via PipelineRunManager.cancel_run() and stops cleanly between nodes"
    - "A pipeline exceeding its timeout is terminated and its status is set to 'timeout' in the database"
    - "Cancelled and timed-out pipelines do not leave zombie tasks or locked resources"
    - "Validation tests cover: cycle rejection, valid DAG acceptance, invalid node types, dangling edges, missing agent config"
    - "Execution tests cover: topological order, output passing, condition branching, retry with backoff, circuit breaker tripping, cancellation, timeout"
  artifacts:
    - path: "gathering/orchestration/pipeline/executor.py"
      provides: "PipelineRunManager class managing active runs with cancellation and timeout"
      contains: "class PipelineRunManager"
    - path: "gathering/orchestration/__init__.py"
      provides: "Updated exports including pipeline package classes"
      contains: "PipelineExecutor"
    - path: "tests/test_pipeline_validation.py"
      provides: "Comprehensive validation tests (cycle detection, schema validation, edge validation)"
      min_lines: 80
    - path: "tests/test_pipeline_execution.py"
      provides: "Comprehensive execution tests (DAG traversal, node dispatch, retry, circuit breaker, cancel, timeout)"
      min_lines: 150
  key_links:
    - from: "gathering/orchestration/pipeline/executor.py"
      to: "asyncio"
      via: "PipelineRunManager uses asyncio.Task for managed execution with timeout"
      pattern: "asyncio\\.create_task|asyncio\\.timeout"
    - from: "tests/test_pipeline_execution.py"
      to: "gathering/orchestration/pipeline/executor.py"
      via: "tests instantiate PipelineExecutor and PipelineRunManager"
      pattern: "PipelineExecutor|PipelineRunManager"
    - from: "tests/test_pipeline_validation.py"
      to: "gathering/orchestration/pipeline/validator.py"
      via: "tests call validate_pipeline_dag and get_execution_order"
      pattern: "validate_pipeline_dag|get_execution_order"
---

<objective>
Add pipeline cancellation and timeout enforcement, update orchestration exports, and write comprehensive tests covering the entire pipeline execution engine -- validation, execution, retry, circuit breaker, cancellation, and timeout.

Purpose: Completes FEAT-04 (cancellation + timeout) and TEST-02 (comprehensive pipeline tests). Without tests, we can't prove the engine works. Without cancellation/timeout, runaway pipelines can't be stopped.
Output: PipelineRunManager for managed execution, full test suites, updated package exports.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-pipeline-execution-engine/02-RESEARCH.md
@.planning/phases/02-pipeline-execution-engine/02-01-SUMMARY.md
@.planning/phases/02-pipeline-execution-engine/02-02-SUMMARY.md

Source files:
@gathering/orchestration/pipeline/__init__.py
@gathering/orchestration/pipeline/executor.py
@gathering/orchestration/pipeline/validator.py
@gathering/orchestration/pipeline/models.py
@gathering/orchestration/pipeline/nodes.py
@gathering/orchestration/pipeline/circuit_breaker.py
@gathering/orchestration/__init__.py
@gathering/orchestration/background.py (reference: BackgroundTaskRunner cancellation pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add PipelineRunManager for cancellation and timeout, update exports</name>
  <files>
    gathering/orchestration/pipeline/executor.py
    gathering/orchestration/pipeline/__init__.py
    gathering/orchestration/__init__.py
  </files>
  <action>
**executor.py** -- Add `PipelineRunManager` class after the existing `PipelineExecutor`:

```python
class PipelineRunManager:
    """Manages active pipeline runs with cancellation and timeout enforcement.

    Tracks running pipelines so they can be cancelled by run_id.
    Enforces per-pipeline timeout using asyncio.timeout.
    Cleans up resources on cancellation/timeout.
    """

    def __init__(self):
        self._running: dict[int, asyncio.Task] = {}        # run_id -> asyncio.Task
        self._executors: dict[int, PipelineExecutor] = {}   # run_id -> executor

    @property
    def active_runs(self) -> list[int]:
        """Return IDs of currently running pipelines."""
        return [rid for rid, task in self._running.items() if not task.done()]

    async def start_run(
        self,
        run_id: int,
        executor: PipelineExecutor,
        timeout_seconds: int = 3600,
        trigger_data: Optional[dict] = None,
        max_retries: int = 3,
        backoff_base: float = 1.0,
        backoff_max: float = 60.0,
    ) -> asyncio.Task:
        """Start a pipeline run with timeout enforcement."""
        self._executors[run_id] = executor

        async def _run_with_timeout():
            try:
                async with asyncio.timeout(timeout_seconds):
                    result = await executor.execute(
                        run_id=run_id,
                        trigger_data=trigger_data,
                        max_retries=max_retries,
                        backoff_base=backoff_base,
                        backoff_max=backoff_max,
                    )
                    return result
            except TimeoutError:
                executor.request_cancel()  # Cooperative cancel
                result = {"status": "timeout", "error": f"Pipeline exceeded {timeout_seconds}s timeout"}
                # Emit timeout event
                await executor._emit_event(
                    EventType.PIPELINE_RUN_TIMEOUT,
                    {"run_id": run_id, "pipeline_id": executor.pipeline_id, "timeout_seconds": timeout_seconds}
                )
                return result
            except asyncio.CancelledError:
                executor.request_cancel()
                return {"status": "cancelled", "error": "Pipeline run was cancelled"}
            finally:
                self._running.pop(run_id, None)
                self._executors.pop(run_id, None)

        task = asyncio.create_task(_run_with_timeout())
        self._running[run_id] = task
        return task

    async def cancel_run(self, run_id: int) -> bool:
        """Cancel a running pipeline. Returns True if cancellation was initiated."""
        executor = self._executors.get(run_id)
        if executor:
            executor.request_cancel()  # Cooperative first

        task = self._running.get(run_id)
        if task and not task.done():
            task.cancel()  # Force cancellation
            return True
        return False

    async def cancel_all(self) -> int:
        """Cancel all running pipelines. Returns count of cancelled runs."""
        cancelled = 0
        for run_id in list(self._running.keys()):
            if await self.cancel_run(run_id):
                cancelled += 1
        return cancelled

    def is_running(self, run_id: int) -> bool:
        """Check if a pipeline run is still active."""
        task = self._running.get(run_id)
        return task is not None and not task.done()
```

Add a module-level singleton factory:

```python
_run_manager: Optional[PipelineRunManager] = None

def get_run_manager() -> PipelineRunManager:
    """Get or create the singleton PipelineRunManager."""
    global _run_manager
    if _run_manager is None:
        _run_manager = PipelineRunManager()
    return _run_manager
```

**__init__.py** (pipeline package) -- Add exports: `PipelineRunManager`, `get_run_manager`.

**__init__.py** (orchestration package) -- Add pipeline imports at the bottom of the existing imports:

```python
from gathering.orchestration.pipeline import (
    PipelineExecutor,
    PipelineRunManager,
    PipelineDefinition,
    PipelineNode,
    PipelineEdge,
    CircuitBreaker,
    validate_pipeline_dag,
    get_execution_order,
    parse_pipeline_definition,
    get_run_manager,
)
```

Add these to the `__all__` list as well.
  </action>
  <verify>
Run: `python -c "from gathering.orchestration.pipeline import PipelineRunManager, get_run_manager; print('PipelineRunManager imports OK')"`
Run: `python -c "from gathering.orchestration import PipelineExecutor, PipelineRunManager, validate_pipeline_dag; print('orchestration exports OK')"`
  </verify>
  <done>PipelineRunManager tracks active runs, supports cancellation by run_id, enforces per-pipeline timeout via asyncio.timeout, cleans up resources. Pipeline classes exported from both pipeline package and orchestration package.</done>
</task>

<task type="auto">
  <name>Task 2: Write comprehensive pipeline validation and execution tests</name>
  <files>
    tests/test_pipeline_validation.py
    tests/test_pipeline_execution.py
  </files>
  <action>
**test_pipeline_validation.py** -- Tests for DAG validation and models:

Use `pytest` with standard assertions. No database needed -- validation is pure logic.

Test cases:
1. `test_valid_linear_pipeline`: 3 nodes (trigger -> agent -> action), no errors
2. `test_valid_branching_pipeline`: trigger -> 2 parallel agents -> merge action, no errors
3. `test_cycle_detection_simple`: A -> B -> A, expect cycle error
4. `test_cycle_detection_complex`: A -> B -> C -> D -> B (cycle deeper in graph), expect cycle error
5. `test_empty_pipeline_rejected`: No nodes, expect error
6. `test_invalid_node_type`: Node with type "invalid", expect error
7. `test_dangling_edge_source`: Edge from non-existent node, expect error
8. `test_dangling_edge_target`: Edge to non-existent node, expect error
9. `test_agent_node_missing_config`: Agent node without agent_id in config, expect error or warning
10. `test_execution_order_linear`: A -> B -> C returns ["A", "B", "C"]
11. `test_execution_order_branching`: Trigger -> (Agent1, Agent2) -> Action. Trigger comes first, Action comes last. Agent1 and Agent2 can be in either order.
12. `test_parse_pipeline_definition_from_jsonb`: Parse raw dicts (matching JSONB format from DB) into PipelineDefinition
13. `test_parse_pipeline_definition_invalid`: Raw dicts with missing fields raise ValueError
14. `test_pipeline_edge_from_alias`: PipelineEdge "from"/"to" JSON aliases work correctly

Helper: Create a `make_pipeline(nodes_spec, edges_spec)` fixture/helper that builds PipelineDefinition from simplified specs.

**test_pipeline_execution.py** -- Tests for executor, node dispatch, retry, circuit breaker, cancellation, timeout:

Use `pytest` with `pytest.mark.asyncio` for async tests. Mock the DatabaseService (following Phase 1 pattern from test_auth_persistence.py). Mock agent registry for agent node tests.

Test cases:

*Execution basics:*
1. `test_execute_linear_pipeline`: 3-node linear pipeline executes all nodes in order, returns completed status
2. `test_execute_output_passing`: Agent node output is available as input to downstream node
3. `test_execute_trigger_passes_data`: Trigger data is available to downstream nodes

*Node dispatch:*
4. `test_dispatch_trigger_node`: Returns inputs unchanged
5. `test_dispatch_agent_node_simulated`: Without agent registry, returns simulated result
6. `test_dispatch_condition_true`: Condition "true" returns {"result": True}
7. `test_dispatch_condition_false`: Condition "false" returns {"result": False}
8. `test_dispatch_action_node`: Action node returns action metadata
9. `test_dispatch_delay_node`: Delay node sleeps and returns inputs (mock asyncio.sleep for speed)
10. `test_dispatch_unknown_type`: Unknown node type raises NodeConfigError

*Condition branching:*
11. `test_condition_false_skips_downstream`: Trigger -> Condition(false) -> Agent. Agent is skipped.
12. `test_condition_true_executes_downstream`: Trigger -> Condition(true) -> Agent. Agent executes.

*Retry and circuit breaker:*
13. `test_node_retry_on_failure`: Node fails twice then succeeds on third try. Should complete.
14. `test_node_retry_exhaustion`: Node fails all retries. Pipeline fails.
15. `test_circuit_breaker_trips_after_threshold`: After threshold failures, breaker.can_execute() returns False
16. `test_circuit_breaker_recovery`: After recovery timeout, breaker transitions to HALF_OPEN and allows test request
17. `test_circuit_breaker_reset_on_success`: Success in HALF_OPEN transitions to CLOSED

*Cancellation and timeout:*
18. `test_pipeline_cancellation`: Start pipeline, request cancel, pipeline returns cancelled status
19. `test_pipeline_timeout`: Pipeline with very short timeout (0.1s) and a delay node (10s) times out
20. `test_run_manager_cancel_run`: PipelineRunManager.cancel_run() stops a running pipeline
21. `test_run_manager_active_runs`: start_run adds to active_runs, completion removes it
22. `test_cancel_leaves_no_zombies`: After cancellation, active_runs is empty

*Event emission:*
23. `test_events_emitted_during_execution`: Mock event_bus, verify PIPELINE_RUN_STARTED, PIPELINE_NODE_STARTED, PIPELINE_NODE_COMPLETED, PIPELINE_RUN_COMPLETED are emitted in order

For mocking:
- Use `unittest.mock.AsyncMock` for event_bus and agent calls
- Use `unittest.mock.MagicMock` for DatabaseService (with `execute` and `execute_one` methods)
- Use `unittest.mock.patch` for `asyncio.sleep` in delay node tests (to avoid actual waits)

For retry tests, use a counter to track call attempts:
```python
call_count = 0
async def flaky_dispatch(node, inputs, context):
    nonlocal call_count
    call_count += 1
    if call_count < 3:
        raise NodeExecutionError("transient failure")
    return {"result": "success"}
```

Patch `dispatch_node` in the executor module to control behavior.
  </action>
  <verify>
Run: `python -m pytest tests/test_pipeline_validation.py -v --timeout=30`
Run: `python -m pytest tests/test_pipeline_execution.py -v --timeout=30`
Run: `python -m pytest tests/ -x -q --timeout=30` (full suite, no regressions)
  </verify>
  <done>Validation tests cover cycle detection, node type validation, edge validation, execution order, and JSONB parsing. Execution tests cover topological traversal, output passing, condition branching, retry with backoff, circuit breaker state transitions, cancellation, timeout, and event emission. All tests pass and existing test suite has no regressions.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_pipeline_validation.py -v` -- all validation tests pass
2. `python -m pytest tests/test_pipeline_execution.py -v` -- all execution tests pass
3. `python -m pytest tests/ -x -q --timeout=30` -- full test suite passes with no regressions
4. `python -c "from gathering.orchestration import PipelineExecutor, PipelineRunManager, validate_pipeline_dag"` -- all exports work
5. Pipeline cancellation test completes in <5s (not waiting for actual delays)
6. Timeout test completes in <5s (not waiting for actual timeout)
</verification>

<success_criteria>
- PipelineRunManager correctly manages active runs with cancellation and timeout
- Cancellation stops pipeline cleanly between nodes (cooperative + forced)
- Timeout triggers after configured seconds and sets status to "timeout"
- 14+ validation tests cover all error cases (cycles, types, edges, config)
- 23+ execution tests cover all features (traversal, dispatch, retry, breaker, cancel, timeout, events)
- All existing tests continue to pass (no regressions)
- Pipeline classes are exported from gathering.orchestration package
</success_criteria>

<output>
After completion, create `.planning/phases/02-pipeline-execution-engine/02-03-SUMMARY.md`
</output>
