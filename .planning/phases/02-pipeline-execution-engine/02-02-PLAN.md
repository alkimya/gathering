---
phase: 02-pipeline-execution-engine
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - gathering/orchestration/pipeline/executor.py
  - gathering/orchestration/pipeline/nodes.py
  - gathering/orchestration/pipeline/circuit_breaker.py
  - gathering/orchestration/pipeline/__init__.py
  - gathering/api/routers/pipelines.py
autonomous: true

must_haves:
  truths:
    - "A pipeline with multiple connected nodes executes in topological order -- each node runs its handler and passes output to downstream nodes"
    - "Agent nodes dispatch to AgentHandle/AgentWrapper for real LLM execution"
    - "Condition nodes evaluate their expression and downstream false-branch nodes are skipped"
    - "A failing node retries with exponential backoff via tenacity up to max_retries_per_node"
    - "After retry exhaustion a node's circuit breaker trips to OPEN state and subsequent calls fail fast"
    - "The API run_pipeline endpoint calls the real executor instead of the stub"
  artifacts:
    - path: "gathering/orchestration/pipeline/executor.py"
      provides: "PipelineExecutor class with topological DAG execution, output passing, event emission"
      contains: "class PipelineExecutor"
    - path: "gathering/orchestration/pipeline/nodes.py"
      provides: "Node type dispatchers: dispatch_node() with handlers for trigger, agent, condition, action, parallel, delay"
      contains: "async def dispatch_node"
    - path: "gathering/orchestration/pipeline/circuit_breaker.py"
      provides: "CircuitBreaker class with CLOSED/OPEN/HALF_OPEN state machine"
      contains: "class CircuitBreaker"
    - path: "gathering/api/routers/pipelines.py"
      provides: "run_pipeline endpoint wired to real PipelineExecutor"
      contains: "PipelineExecutor"
  key_links:
    - from: "gathering/orchestration/pipeline/executor.py"
      to: "gathering/orchestration/pipeline/validator.py"
      via: "validates pipeline before execution"
      pattern: "validate_pipeline_dag"
    - from: "gathering/orchestration/pipeline/executor.py"
      to: "gathering/orchestration/pipeline/nodes.py"
      via: "dispatches each node to type-specific handler"
      pattern: "dispatch_node"
    - from: "gathering/orchestration/pipeline/executor.py"
      to: "gathering/orchestration/events.py"
      via: "emits pipeline lifecycle events"
      pattern: "EventType\\.PIPELINE_"
    - from: "gathering/orchestration/pipeline/nodes.py"
      to: "gathering/orchestration/circle.py"
      via: "agent nodes use AgentHandle for execution"
      pattern: "AgentHandle|process_message"
    - from: "gathering/api/routers/pipelines.py"
      to: "gathering/orchestration/pipeline/executor.py"
      via: "run_pipeline creates PipelineExecutor and starts execution"
      pattern: "PipelineExecutor"
---

<objective>
Build the pipeline execution engine -- topological DAG traversal that runs each node's handler (agent, condition, action, etc.), passes outputs between nodes, integrates retry with exponential backoff via tenacity, circuit breaker per node, and replaces the stubbed API endpoint with real execution.

Purpose: This is the core of Phase 2 -- turning pipeline CRUD into pipeline execution. Without this, pipelines are just stored JSON.
Output: Working executor that traverses DAGs, dispatches nodes, retries failures, and is callable from the API.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-pipeline-execution-engine/02-RESEARCH.md
@.planning/phases/02-pipeline-execution-engine/02-01-SUMMARY.md

Source files:
@gathering/orchestration/pipeline/__init__.py
@gathering/orchestration/pipeline/models.py
@gathering/orchestration/pipeline/validator.py
@gathering/orchestration/events.py
@gathering/orchestration/background.py (reference: BackgroundTaskRunner patterns for event emission, cancellation)
@gathering/orchestration/circle.py (reference: AgentHandle interface)
@gathering/api/routers/pipelines.py
@gathering/api/dependencies.py (reference: DatabaseService, get_database_service)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pipeline executor, node dispatchers, and circuit breaker</name>
  <files>
    gathering/orchestration/pipeline/executor.py
    gathering/orchestration/pipeline/nodes.py
    gathering/orchestration/pipeline/circuit_breaker.py
    gathering/orchestration/pipeline/__init__.py
  </files>
  <action>
**circuit_breaker.py** -- Lightweight circuit breaker (~50 lines):

- `CircuitState` enum: CLOSED, OPEN, HALF_OPEN
- `CircuitBreaker` dataclass:
  - `failure_threshold: int = 5`
  - `recovery_timeout: float = 60.0` (seconds)
  - `state: CircuitState = CircuitState.CLOSED`
  - `failure_count: int = 0`
  - `last_failure_time: float = 0.0`
  - Methods: `can_execute() -> bool`, `record_success()`, `record_failure()`, `is_open` property
  - Use `time.monotonic()` for timing (not wall clock)
  - State transitions: failure_count >= threshold -> OPEN; monotonic elapsed >= recovery_timeout -> HALF_OPEN; success in HALF_OPEN -> CLOSED; failure in HALF_OPEN -> OPEN

**nodes.py** -- Node type dispatchers:

- `class NodeExecutionError(Exception)`: Custom exception for node failures (retryable).
- `class NodeConfigError(Exception)`: Custom exception for config errors (NOT retryable).
- `async def dispatch_node(node: PipelineNode, inputs: dict[str, Any], context: dict) -> dict`:
  - `context` contains: `db` (DatabaseService), `event_bus` (EventBus, optional), `agent_registry` (optional)
  - Dispatch by `node.type`:
    - **trigger**: Return inputs unchanged (passthrough)
    - **agent**: Get `agent_id` and `task` from node.config. Look up agent via context["agent_registry"] if available; if not, return `{"result": f"Agent {agent_id} task: {task}", "agent_id": agent_id, "simulated": True}` (graceful degradation when no registry). When registry is available, call agent's async process_message. Wrap failures in NodeExecutionError.
    - **condition**: Evaluate `config["condition"]` against inputs. For safety, do NOT use `eval()`. Instead, support simple comparisons: "true", "false", or check if a specific input key is truthy (e.g., `"input.result"` checks `inputs[predecessor_id]["result"]`). Return `{"result": bool}`.
    - **action**: Get `config["action"]` (action type string). For now, log the action and return `{"action": action_type, "executed": True, "inputs": summarized_inputs}`. Real action dispatch will be extended in Phase 3.
    - **parallel**: Passthrough (fan-out handled by executor topology)
    - **delay**: `await asyncio.sleep(config.get("seconds", 0))`, return inputs
    - **unknown**: Raise NodeConfigError

**executor.py** -- Core execution engine:

- `class PipelineExecutor`:
  - `__init__(self, pipeline_id: int, definition: PipelineDefinition, db: DatabaseService, event_bus: Optional[EventBus] = None, agent_registry: Optional[Any] = None)`:
    - Store params. Create `_cancel_requested = False`, `_node_outputs: dict[str, Any] = {}`, `_circuit_breakers: dict[str, CircuitBreaker] = {}`.
    - Initialize circuit breakers per node from pipeline config: `failure_threshold` from definition or default 5.

  - `def request_cancel(self)`: Set `_cancel_requested = True`

  - `async def execute(self, run_id: int, trigger_data: Optional[dict] = None, max_retries: int = 3, backoff_base: float = 1.0, backoff_max: float = 60.0) -> dict`:
    1. Validate pipeline using `validate_pipeline_dag(self.definition)`. If errors, return `{"status": "failed", "error": "Validation failed: " + "; ".join(errors)}`.
    2. Emit `PIPELINE_RUN_STARTED` event.
    3. Get execution order via `get_execution_order(self.definition)`.
    4. Build predecessor map from edges: `predecessors[node_id] = set(source_ids)`.
    5. Set trigger node outputs to `trigger_data or {}`.
    6. Track `skipped_nodes: set[str]` for condition false-branch handling.
    7. Loop through nodes in topological order:
       a. Check `_cancel_requested` -> return `{"status": "cancelled"}` + emit PIPELINE_RUN_CANCELLED
       b. Check if node should be skipped: if ALL predecessor paths lead through a skipped condition node (i.e., all predecessors are in skipped_nodes), skip this node too. Mark in skipped_nodes.
       c. For non-skipped nodes, emit `PIPELINE_NODE_STARTED`.
       d. Gather inputs: `{pred_id: self._node_outputs[pred_id] for pred_id in predecessors[node_id] if pred_id not in skipped_nodes}`.
       e. Check circuit breaker: if `not breaker.can_execute()`, mark node failed, emit PIPELINE_NODE_FAILED, continue.
       f. Execute with retry using tenacity:
          ```python
          @retry(stop=stop_after_attempt(max_retries), wait=wait_exponential(multiplier=backoff_base, max=backoff_max), retry=retry_if_exception_type(NodeExecutionError), before_sleep=lambda rs: emit_retrying(rs))
          async def _run_node():
              return await dispatch_node(node, inputs, context)
          ```
       g. On success: `breaker.record_success()`, store output in `_node_outputs`, emit PIPELINE_NODE_COMPLETED. Record NodeExecutionResult.
       h. On failure (RetryError after exhaustion): `breaker.record_failure()`, emit PIPELINE_NODE_FAILED. If node is critical (non-condition), return `{"status": "failed", ...}`.
       i. For condition nodes: if result["result"] is False, add all downstream-only successors to skipped_nodes.
    8. Return `{"status": "completed", "outputs": self._node_outputs, "node_results": [...]}`.
    9. Emit PIPELINE_RUN_COMPLETED or PIPELINE_RUN_FAILED in finally block.

  - `async def _persist_node_run(self, run_id: int, result: NodeExecutionResult)`:
    Insert into `circle.pipeline_node_runs` using `self.db.execute()` with parameterized query.

  - `async def _emit_event(self, event_type: EventType, data: dict)`:
    If `self.event_bus`, call `await self.event_bus.emit(event_type=event_type, data=data, source_agent_id=None)`. Wrap in try/except to never fail the pipeline due to event emission.

Install tenacity: `pip install "tenacity>=8.2,<9.0"` -- it's declared in pyproject.toml but may not be installed.

Update `__init__.py` to export: `PipelineExecutor`, `CircuitBreaker`, `CircuitState`, `NodeExecutionError`, `NodeConfigError`, `dispatch_node`.
  </action>
  <verify>
Run: `pip install "tenacity>=8.2,<9.0"` (ensure installed)
Run: `python -c "from gathering.orchestration.pipeline import PipelineExecutor, CircuitBreaker, dispatch_node; print('imports OK')"`
Run: `python -c "
from gathering.orchestration.pipeline.circuit_breaker import CircuitBreaker, CircuitState
cb = CircuitBreaker(failure_threshold=3)
assert cb.can_execute()
cb.record_failure(); cb.record_failure(); cb.record_failure()
assert cb.is_open
print('circuit breaker OK')
"`
  </verify>
  <done>PipelineExecutor traverses DAG in topological order, dispatches nodes via dispatch_node(), passes outputs between nodes, handles condition branching, retries failures with tenacity exponential backoff, and trips circuit breakers after retry exhaustion. CircuitBreaker implements CLOSED/OPEN/HALF_OPEN state machine.</done>
</task>

<task type="auto">
  <name>Task 2: Wire pipeline executor into API router</name>
  <files>
    gathering/api/routers/pipelines.py
  </files>
  <action>
Replace the stubbed execution in the `run_pipeline` endpoint (around lines 419-451) with real executor integration.

**Changes to `run_pipeline()` function:**

1. After creating the pipeline_run record and updating run_count (keep existing DB operations for creating the run), replace the TODO block (lines 419-451) with:

2. Parse the pipeline definition:
   ```python
   from gathering.orchestration.pipeline import parse_pipeline_definition, PipelineExecutor
   import json

   nodes_data = pipeline['nodes'] if isinstance(pipeline['nodes'], list) else json.loads(pipeline['nodes'] or '[]')
   edges_data = pipeline['edges'] if isinstance(pipeline['edges'], list) else json.loads(pipeline['edges'] or '[]')

   try:
       definition = parse_pipeline_definition(nodes_data, edges_data)
   except ValueError as e:
       # Validation failed -- mark run as failed and return
       result = db.execute_one("""
           UPDATE circle.pipeline_runs
           SET status = 'failed', completed_at = CURRENT_TIMESTAMP, error_message = %(error)s
           WHERE id = %(id)s RETURNING *
       """, {'id': run['id'], 'error': str(e)})
       db.execute("UPDATE circle.pipelines SET error_count = error_count + 1 WHERE id = %(id)s", {'id': pipeline_id})
       return _serialize_row(result)
   ```

3. Create the executor and run in background:
   ```python
   executor = PipelineExecutor(
       pipeline_id=pipeline_id,
       definition=definition,
       db=db,
   )

   # Get retry config from pipeline (or use defaults)
   max_retries = pipeline.get('max_retries_per_node', 3)
   timeout_seconds = pipeline.get('timeout_seconds', 3600)
   ```

4. For now, execute synchronously in the request (the endpoint is already async). Wrap in try/except:
   ```python
   import asyncio

   try:
       exec_result = await asyncio.wait_for(
           executor.execute(
               run_id=run['id'],
               trigger_data=trigger_data or {},
               max_retries=max_retries,
           ),
           timeout=timeout_seconds,
       )
   except asyncio.TimeoutError:
       exec_result = {"status": "timeout", "error": f"Pipeline exceeded {timeout_seconds}s timeout"}
   except Exception as e:
       exec_result = {"status": "failed", "error": str(e)}
   ```

5. Update the run record based on result:
   ```python
   final_status = exec_result.get("status", "failed")
   error_msg = exec_result.get("error")

   result = db.execute_one("""
       UPDATE circle.pipeline_runs
       SET status = %(status)s, completed_at = CURRENT_TIMESTAMP,
           error_message = %(error)s,
           logs = %(logs)s::jsonb
       WHERE id = %(id)s RETURNING *
   """, {
       'id': run['id'],
       'status': final_status,
       'error': error_msg,
       'logs': json.dumps(exec_result.get("node_results", [])),
   })

   # Update pipeline success/error counts
   if final_status == 'completed':
       db.execute("UPDATE circle.pipelines SET success_count = success_count + 1 WHERE id = %(id)s", {'id': pipeline_id})
   else:
       db.execute("UPDATE circle.pipelines SET error_count = error_count + 1 WHERE id = %(id)s", {'id': pipeline_id})
   ```

6. Add a new validation endpoint BEFORE the run endpoint:
   ```python
   @router.post("/{pipeline_id}/validate", response_model=dict)
   async def validate_pipeline(pipeline_id: int, db: DatabaseService = Depends(get_database_service)):
       """Validate a pipeline's DAG structure without executing it."""
       pipeline = db.execute_one("SELECT * FROM circle.pipelines WHERE id = %(id)s", {'id': pipeline_id})
       if not pipeline:
           raise HTTPException(status_code=404, detail="Pipeline not found")

       nodes_data = pipeline['nodes'] if isinstance(pipeline['nodes'], list) else json.loads(pipeline['nodes'] or '[]')
       edges_data = pipeline['edges'] if isinstance(pipeline['edges'], list) else json.loads(pipeline['edges'] or '[]')

       try:
           definition = parse_pipeline_definition(nodes_data, edges_data)
           from gathering.orchestration.pipeline import validate_pipeline_dag
           errors = validate_pipeline_dag(definition)
           return {"valid": len(errors) == 0, "errors": errors}
       except ValueError as e:
           return {"valid": False, "errors": [str(e)]}
   ```

**Important:** Keep all existing CRUD endpoints unchanged. Only modify the `run_pipeline` function body and add the `validate_pipeline` endpoint. Do NOT change `_ensure_table_exists` -- the migration handles schema extensions.
  </action>
  <verify>
Run: `python -c "from gathering.api.routers.pipelines import router; print('router imports OK')"`
Run: `python -m pytest tests/ -x -q --timeout=30 -k 'not test_auth_persistence and not test_sql_security'` (existing tests pass)
Grep: Confirm no TODO about "Actually execute the pipeline" remains in pipelines.py
  </verify>
  <done>The run_pipeline API endpoint creates a PipelineExecutor, validates the DAG, executes nodes for real with retry/backoff, and records results. A new /validate endpoint checks DAG structure without executing. The stubbed execution code is fully replaced.</done>
</task>

</tasks>

<verification>
1. `python -c "from gathering.orchestration.pipeline import PipelineExecutor, CircuitBreaker"` succeeds
2. The stubbed TODO at line 419 of pipelines.py is replaced with real executor call
3. Circuit breaker transitions: CLOSED -> OPEN after threshold failures, OPEN -> HALF_OPEN after timeout, HALF_OPEN -> CLOSED on success
4. Node dispatch handles all 6 types: trigger, agent, condition, action, parallel, delay
5. Condition false-branch skips downstream nodes
6. Existing tests still pass: `python -m pytest tests/ -x -q --timeout=30`
</verification>

<success_criteria>
- PipelineExecutor.execute() runs nodes in topological order and returns completed/failed/cancelled status
- dispatch_node() handles all 6 node types with appropriate logic
- CircuitBreaker correctly implements CLOSED/OPEN/HALF_OPEN state machine
- tenacity retry wraps node execution with configurable exponential backoff
- API run_pipeline endpoint uses real executor instead of stub
- New /validate endpoint available for DAG validation without execution
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-pipeline-execution-engine/02-02-SUMMARY.md`
</output>
