---
phase: 02-pipeline-execution-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - gathering/orchestration/pipeline/__init__.py
  - gathering/orchestration/pipeline/models.py
  - gathering/orchestration/pipeline/validator.py
  - gathering/orchestration/events.py
  - gathering/db/migrations/007_pipeline_execution.sql
autonomous: true

must_haves:
  truths:
    - "A pipeline with a cycle is rejected at validation time with a CycleError-derived message before any node executes"
    - "A pipeline with invalid node types or dangling edge references is rejected with specific error messages"
    - "Pipeline node configuration is validated against Pydantic models with discriminated union on node type"
    - "Pipeline event types exist in EventType enum for run lifecycle and per-node lifecycle"
    - "Database schema supports per-node execution tracking with pipeline_node_runs table"
  artifacts:
    - path: "gathering/orchestration/pipeline/__init__.py"
      provides: "Package exports for validator, models"
    - path: "gathering/orchestration/pipeline/models.py"
      provides: "Pydantic models: PipelineDefinition, PipelineNode (discriminated union by type), PipelineEdge, NodeConfig variants"
      contains: "class PipelineNode"
    - path: "gathering/orchestration/pipeline/validator.py"
      provides: "validate_pipeline_dag() using graphlib.TopologicalSorter with CycleError handling"
      contains: "graphlib.TopologicalSorter"
    - path: "gathering/orchestration/events.py"
      provides: "Pipeline-specific EventType entries"
      contains: "PIPELINE_RUN_STARTED"
    - path: "gathering/db/migrations/007_pipeline_execution.sql"
      provides: "Schema extensions: pipeline config columns, pipeline_node_runs table, timeout status"
      contains: "pipeline_node_runs"
  key_links:
    - from: "gathering/orchestration/pipeline/validator.py"
      to: "graphlib"
      via: "TopologicalSorter for cycle detection"
      pattern: "graphlib\\.TopologicalSorter"
    - from: "gathering/orchestration/pipeline/validator.py"
      to: "gathering/orchestration/pipeline/models.py"
      via: "validates PipelineDefinition model"
      pattern: "PipelineDefinition|PipelineNode"
---

<objective>
Create the pipeline validation and data model foundation -- Pydantic models for pipeline/node/edge configuration, DAG validation with cycle detection using graphlib.TopologicalSorter, pipeline-specific EventType entries, and the database schema extension for per-node execution tracking.

Purpose: Every subsequent plan (executor, retry, cancellation) depends on validated models and schema. Validation must reject bad pipelines BEFORE execution starts.
Output: Pipeline package with models + validator, extended EventTypes, DB migration for node-level tracking.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-pipeline-execution-engine/02-RESEARCH.md

Source files:
@gathering/orchestration/events.py
@gathering/api/routers/pipelines.py (for JSONB node/edge structure reference)
@gathering/orchestration/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pipeline package with Pydantic models and DAG validator</name>
  <files>
    gathering/orchestration/pipeline/__init__.py
    gathering/orchestration/pipeline/models.py
    gathering/orchestration/pipeline/validator.py
  </files>
  <action>
Create `gathering/orchestration/pipeline/` package directory.

**models.py** -- Pydantic models matching the existing JSONB node/edge structure in `circle.pipelines`:

- `PipelineEdge`: fields `id` (str), `from_node` (str, alias "from"), `to_node` (str, alias "to"), `condition` (Optional[str])
- `PipelineNode`: fields `id` (str), `type` (Literal["trigger", "agent", "condition", "action", "parallel", "delay"]), `name` (str), `config` (dict), `position` (Optional[dict]), `next` (Optional[list[str]])
- `AgentNodeConfig`: fields `agent_id` (str), `task` (str)
- `ConditionNodeConfig`: fields `condition` (str, default="true")
- `ActionNodeConfig`: fields `action` (str), plus arbitrary config via model_config extra="allow"
- `DelayNodeConfig`: fields `seconds` (float, ge=0, default=0)
- `PipelineDefinition`: fields `nodes` (list[PipelineNode]), `edges` (list[PipelineEdge]). Add a `node_map` property returning dict[str, PipelineNode].
- `NodeExecutionResult`: fields `node_id` (str), `status` (Literal["completed", "failed", "skipped", "cancelled"]), `output` (Optional[dict]), `error` (Optional[str]), `duration_ms` (int, default=0), `retry_count` (int, default=0)

Use `model_config = ConfigDict(populate_by_name=True)` on PipelineEdge so "from"/"to" JSON keys map to from_node/to_node Python attributes.

**validator.py** -- Pipeline DAG validation:

- `validate_pipeline_dag(definition: PipelineDefinition) -> list[str]`: Returns list of error strings (empty = valid).
  1. Check at least one node exists
  2. Check all node types are valid (the Pydantic model handles this, but double-check for raw dict input)
  3. Check all edge endpoints reference existing node IDs
  4. Check for orphan nodes (nodes with no edges that aren't trigger nodes -- warn, don't error)
  5. Build predecessor graph: `{node_id: set(predecessor_ids)}` from edges
  6. Use `graphlib.TopologicalSorter(graph)` and call `list(ts.static_order())`. Catch `graphlib.CycleError` and append cycle info to errors
  7. Validate node-type-specific config: agent nodes must have `agent_id` in config, condition nodes must have `condition` in config

- `get_execution_order(definition: PipelineDefinition) -> list[str]`: Returns node IDs in topological order. Raises ValueError if graph has cycles. This is used by the executor (Plan 02-02).

- `parse_pipeline_definition(nodes_json: list[dict], edges_json: list[dict]) -> PipelineDefinition`: Parses raw JSONB lists into validated PipelineDefinition. Raises ValueError with details if parsing fails.

**__init__.py** -- Export: `validate_pipeline_dag`, `get_execution_order`, `parse_pipeline_definition`, `PipelineDefinition`, `PipelineNode`, `PipelineEdge`, `NodeExecutionResult`.

Import pattern: `from gathering.orchestration.pipeline import validate_pipeline_dag, PipelineDefinition`
  </action>
  <verify>
Run: `python -c "from gathering.orchestration.pipeline import validate_pipeline_dag, PipelineDefinition, get_execution_order; print('imports OK')"`
Run: `python -c "
from gathering.orchestration.pipeline.validator import validate_pipeline_dag
from gathering.orchestration.pipeline.models import PipelineDefinition, PipelineNode, PipelineEdge
nodes = [PipelineNode(id='a', type='trigger', name='Start', config={}), PipelineNode(id='b', type='agent', name='Run', config={'agent_id': '1', 'task': 'test'})]
edges = [PipelineEdge(id='e1', from_node='a', to_node='b')]
defn = PipelineDefinition(nodes=nodes, edges=edges)
errors = validate_pipeline_dag(defn)
assert errors == [], f'Unexpected errors: {errors}'
print('validation OK')
"`
  </verify>
  <done>Pipeline models parse the existing JSONB node/edge format. validate_pipeline_dag rejects cycles (via graphlib.CycleError), invalid node types, and dangling edges. get_execution_order returns topological node order.</done>
</task>

<task type="auto">
  <name>Task 2: Add pipeline EventTypes and create DB migration for node-level tracking</name>
  <files>
    gathering/orchestration/events.py
    gathering/db/migrations/007_pipeline_execution.sql
  </files>
  <action>
**events.py** -- Add pipeline-specific EventType entries after the existing "Scheduled action events" block (around line 84):

```python
    # Pipeline execution events
    PIPELINE_RUN_STARTED = "pipeline_run.started"
    PIPELINE_RUN_COMPLETED = "pipeline_run.completed"
    PIPELINE_RUN_FAILED = "pipeline_run.failed"
    PIPELINE_RUN_CANCELLED = "pipeline_run.cancelled"
    PIPELINE_RUN_TIMEOUT = "pipeline_run.timeout"
    PIPELINE_NODE_STARTED = "pipeline_node.started"
    PIPELINE_NODE_COMPLETED = "pipeline_node.completed"
    PIPELINE_NODE_FAILED = "pipeline_node.failed"
    PIPELINE_NODE_SKIPPED = "pipeline_node.skipped"
    PIPELINE_NODE_RETRYING = "pipeline_node.retrying"
```

**007_pipeline_execution.sql** -- Schema extensions for pipeline execution:

1. Add execution config columns to `circle.pipelines`:
   - `timeout_seconds INTEGER DEFAULT 3600`
   - `max_retries_per_node INTEGER DEFAULT 3`
   - `retry_backoff_base FLOAT DEFAULT 1.0`
   - `retry_backoff_max FLOAT DEFAULT 60.0`

2. Update `circle.pipeline_runs` status constraint to include 'timeout':
   - DROP the existing CHECK constraint, then ADD new CHECK with ('pending', 'running', 'completed', 'failed', 'cancelled', 'timeout')
   - Add `duration_seconds INTEGER DEFAULT 0` column

3. Create `circle.pipeline_node_runs` table:
   - `id SERIAL PRIMARY KEY`
   - `run_id INTEGER NOT NULL REFERENCES circle.pipeline_runs(id) ON DELETE CASCADE`
   - `node_id VARCHAR(100) NOT NULL`
   - `node_type VARCHAR(50) NOT NULL`
   - `status VARCHAR(50) DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'completed', 'failed', 'skipped', 'cancelled'))`
   - `input_data JSONB DEFAULT '{}'::jsonb`
   - `output_data JSONB DEFAULT '{}'::jsonb`
   - `error_message TEXT`
   - `retry_count INTEGER DEFAULT 0`
   - `started_at TIMESTAMP WITH TIME ZONE`
   - `completed_at TIMESTAMP WITH TIME ZONE`
   - `duration_ms INTEGER DEFAULT 0`
   - Index on `(run_id)` and `(run_id, node_id)`

Use `IF NOT EXISTS` / `IF NOT EXISTS` patterns for idempotency, consistent with existing migrations. Use `ALTER TABLE ... ADD COLUMN IF NOT EXISTS` for column additions.
  </action>
  <verify>
Run: `python -c "from gathering.orchestration.events import EventType; print(EventType.PIPELINE_RUN_STARTED.value); print(EventType.PIPELINE_NODE_COMPLETED.value)"`
Verify migration file exists: `cat gathering/db/migrations/007_pipeline_execution.sql`
Verify SQL syntax: `python -c "open('gathering/db/migrations/007_pipeline_execution.sql').read(); print('SQL file readable')"`
  </verify>
  <done>EventType enum includes 10 pipeline-specific events (5 run-level, 5 node-level). Migration 007 adds execution config to pipelines table, timeout status to runs, and creates pipeline_node_runs table for per-node tracking.</done>
</task>

</tasks>

<verification>
1. `python -c "from gathering.orchestration.pipeline import validate_pipeline_dag, PipelineDefinition, get_execution_order"` succeeds
2. `python -c "from gathering.orchestration.events import EventType; assert hasattr(EventType, 'PIPELINE_RUN_STARTED')"` succeeds
3. Cycle detection works: a graph with A->B->A is rejected
4. Valid DAG passes: a graph with A->B->C returns no errors
5. Existing tests still pass: `python -m pytest tests/ -x -q --timeout=30` (no regressions)
</verification>

<success_criteria>
- Pipeline models parse raw JSONB node/edge dicts into validated Pydantic models
- validate_pipeline_dag detects cycles, invalid types, dangling edges, and missing required config
- get_execution_order returns correct topological sort
- EventType enum has all 10 pipeline event types
- Migration 007 SQL is syntactically valid and idempotent
- All existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-pipeline-execution-engine/02-01-SUMMARY.md`
</output>
