---
phase: 04-performance-optimization
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - gathering/api/rate_limit.py
  - gathering/api/main.py
  - gathering/api/middleware.py
  - gathering/utils/bounded_lru.py
  - gathering/agents/memory.py
  - gathering/llm/providers.py
  - gathering/orchestration/events.py
  - gathering/rag/embeddings.py
  - gathering/skills/ai/models.py
autonomous: true

must_haves:
  truths:
    - "API endpoints enforce per-endpoint rate limits using slowapi decorators"
    - "Exceeding the rate limit returns 429 Too Many Requests with a Retry-After header"
    - "In-memory caches have configurable size bounds and evict LRU entries when full"
    - "The old hand-rolled RateLimitMiddleware is removed"
  artifacts:
    - path: "gathering/api/rate_limit.py"
      provides: "slowapi limiter configuration with rate tiers"
      contains: "Limiter"
    - path: "gathering/utils/bounded_lru.py"
      provides: "Reusable BoundedLRUDict class for all in-memory caches"
      contains: "class BoundedLRUDict"
    - path: "gathering/api/main.py"
      provides: "slowapi exception handler and limiter state on app"
      contains: "RateLimitExceeded"
  key_links:
    - from: "gathering/api/rate_limit.py"
      to: "slowapi"
      via: "Limiter import and configuration"
      pattern: "from slowapi import Limiter"
    - from: "gathering/api/main.py"
      to: "gathering/api/rate_limit.py"
      via: "app.state.limiter assignment"
      pattern: "app\\.state\\.limiter"
    - from: "gathering/agents/memory.py"
      to: "gathering/utils/bounded_lru.py"
      via: "BoundedLRUDict replacing unbounded Dict caches"
      pattern: "BoundedLRUDict"
---

<objective>
Replace the hand-rolled rate limiter with slowapi per-endpoint tiers and bound all unbounded in-memory caches with LRU eviction.

Purpose: PERF-03 and PERF-05 -- rate limiting protects against abuse and DoS, cache bounding prevents memory exhaustion on long-running servers. The current RateLimitMiddleware uses an unbounded defaultdict(list) that grows forever. Multiple in-memory caches (MemoryService._persona_cache, _project_cache, _session_cache, EmbeddingService._memory_cache, AIModelsSkill._embedding_cache) have no size limits.

Output: slowapi-powered rate limiting with per-endpoint tiers; BoundedLRUDict utility class; all identified unbounded caches retrofitted with size bounds.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-performance-optimization/04-RESEARCH.md
@gathering/api/middleware.py
@gathering/api/main.py
@gathering/agents/memory.py
@gathering/llm/providers.py
@gathering/rag/embeddings.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace RateLimitMiddleware with slowapi per-endpoint rate limiting</name>
  <files>gathering/api/rate_limit.py, gathering/api/main.py, gathering/api/middleware.py</files>
  <action>
1. **Install slowapi**:
   ```bash
   pip install slowapi
   ```
   Add `slowapi>=0.1.9` to `requirements.txt` in the appropriate section.

2. **Create `gathering/api/rate_limit.py`**:
   ```python
   """Per-endpoint rate limiting using slowapi."""
   from slowapi import Limiter
   from slowapi.util import get_remote_address

   # Rate limit tiers
   TIER_AUTH = "5/minute"        # Authentication endpoints (login, register)
   TIER_WRITE = "30/minute"      # Write/mutation endpoints (POST, PUT, DELETE)
   TIER_READ = "120/minute"      # Read endpoints (GET listings)
   TIER_HEALTH = "300/minute"    # Health/status/docs endpoints

   def create_limiter() -> Limiter:
       """Create the rate limiter with optional Redis backend."""
       import os
       storage_uri = os.getenv("REDIS_URL")  # None = in-memory, redis://... = distributed
       return Limiter(
           key_func=get_remote_address,
           storage_uri=storage_uri,
           default_limits=[TIER_READ],  # Default for endpoints without explicit decorator
       )

   # Module-level singleton
   limiter = create_limiter()
   ```

3. **Update `gathering/api/main.py`**:
   - Add imports at top: `from slowapi.errors import RateLimitExceeded` and `from slowapi import _rate_limit_exceeded_handler`
   - In `create_app()`, REPLACE the `RateLimitMiddleware` block (lines ~189-195):
     ```python
     # Rate limiting via slowapi (per-endpoint decorators)
     if enable_rate_limit:
         from gathering.api.rate_limit import limiter
         app.state.limiter = limiter
         app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
     ```
   - Remove the `RateLimitMiddleware` import from the top of main.py
   - Remove the `from gathering.api.middleware import RateLimitMiddleware` line

4. **Do NOT delete `RateLimitMiddleware` from middleware.py** yet -- it is a separate file and other code might reference it. Just stop adding it in `create_app()`. Add a deprecation comment at the top of the class: `# DEPRECATED: Replaced by slowapi in gathering/api/rate_limit.py`

5. **Apply rate limit decorators to auth endpoints** as a demonstration of per-endpoint tiers. In `gathering/api/routers/auth.py`, add the `@limiter.limit(TIER_AUTH)` decorator to the login and register endpoints (if they exist). Ensure each decorated handler has `request: Request` in its signature (required by slowapi). Import from `gathering.api.rate_limit import limiter, TIER_AUTH`.

6. **If auth.py has no route decorators** (auth is middleware-based), skip step 5 -- the default_limits on the limiter will apply globally. The per-endpoint decorators are opt-in and can be added to individual routers incrementally.

7. The `_rate_limit_exceeded_handler` from slowapi returns a 429 response with `Retry-After` header automatically. Verify this by checking slowapi source.
  </action>
  <verify>
    - `pip show slowapi` returns version info
    - `python -c "from gathering.api.rate_limit import limiter, TIER_AUTH, TIER_WRITE, TIER_READ; print('import ok')"` succeeds
    - `python -c "from gathering.api.main import create_app; app = create_app(); print(hasattr(app.state, 'limiter'))"` prints True
    - Grep `create_app` in main.py -- no reference to `RateLimitMiddleware`
    - `python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -20` passes
  </verify>
  <done>slowapi limiter configured with per-endpoint tiers (TIER_AUTH=5/min, TIER_WRITE=30/min, TIER_READ=120/min), wired into FastAPI app via app.state.limiter and RateLimitExceeded handler. Old RateLimitMiddleware no longer added in create_app().</done>
</task>

<task type="auto">
  <name>Task 2: Create BoundedLRUDict and retrofit unbounded in-memory caches</name>
  <files>gathering/utils/bounded_lru.py, gathering/agents/memory.py, gathering/llm/providers.py, gathering/orchestration/events.py, gathering/rag/embeddings.py, gathering/skills/ai/models.py</files>
  <action>
1. **Create `gathering/utils/bounded_lru.py`** with a reusable `BoundedLRUDict`:
   ```python
   """Bounded LRU dictionary for in-memory caches."""
   from collections import OrderedDict
   from typing import TypeVar, Optional

   V = TypeVar("V")

   class BoundedLRUDict(OrderedDict):
       """OrderedDict with configurable max size and LRU eviction.

       On insertion beyond max_size, the least-recently-used entry is evicted.
       On access (__getitem__), the accessed entry is moved to most-recent position.
       """

       def __init__(self, max_size: int = 1000, *args, **kwargs):
           self._max_size = max_size
           super().__init__(*args, **kwargs)

       def __setitem__(self, key, value):
           if key in self:
               self.move_to_end(key)
           super().__setitem__(key, value)
           while len(self) > self._max_size:
               self.popitem(last=False)  # Evict LRU

       def __getitem__(self, key):
           self.move_to_end(key)
           return super().__getitem__(key)

       def get(self, key, default=None):
           if key in self:
               self.move_to_end(key)
               return super().__getitem__(key)
           return default

       @property
       def max_size(self) -> int:
           return self._max_size
   ```

   Also ensure `gathering/utils/__init__.py` exists (create if not).

2. **Retrofit `MemoryService` caches** in `gathering/agents/memory.py` (around line 199):
   - Replace `self._persona_cache: Dict[int, AgentPersona] = {}` with `BoundedLRUDict(max_size=500)`
   - Replace `self._project_cache: Dict[int, ProjectContext] = {}` with `BoundedLRUDict(max_size=100)`
   - Replace `self._session_cache: Dict[int, AgentSession] = {}` with `BoundedLRUDict(max_size=500)`
   - Add import: `from gathering.utils.bounded_lru import BoundedLRUDict`
   - The existing `.get()` and `[key] = value` patterns are compatible with BoundedLRUDict since it inherits from OrderedDict.

3. **Retrofit `EmbeddingService._memory_cache`** in `gathering/rag/embeddings.py` (around line 95):
   - Replace `self._memory_cache: Dict[str, List[float]] = {}` with `BoundedLRUDict(max_size=2000)`
   - Add import: `from gathering.utils.bounded_lru import BoundedLRUDict`

4. **Retrofit `AIModelsSkill._embedding_cache`** in `gathering/skills/ai/models.py` (around line 75):
   - Replace `self._embedding_cache: Dict[str, List[float]] = {}` with `BoundedLRUDict(max_size=2000)`
   - Remove the manual size check at line ~700: `if len(self._embedding_cache) < self.EMBEDDING_CACHE_SIZE:` -- BoundedLRUDict handles eviction automatically.
   - Add import: `from gathering.utils.bounded_lru import BoundedLRUDict`

5. **Verify `LRUCache` in `gathering/llm/providers.py`** (line 73) -- this already has a max_size and manual eviction via OrderedDict. It is already bounded. Leave it as-is but note that its pattern matches BoundedLRUDict. Optionally refactor to use BoundedLRUDict for consistency, but this is low priority -- the existing implementation works correctly.

6. **Verify event history bounds**:
   - `gathering/events/event_bus.py` (line 172): Uses `deque(maxlen=1000)` -- already bounded. No change needed.
   - `gathering/orchestration/events.py` (line 158): Uses `self._history: List[Event] = []` with manual trim at line 225-226. Replace with `deque(maxlen=history_size)` for O(1) append and automatic eviction, matching the pattern in event_bus.py. Add `from collections import deque` if not imported.

7. **TokenBlacklist** in `gathering/api/auth.py` (line 243): Already uses `OrderedDict` with `_cache_max_size` and manual `popitem(last=False)`. Already bounded from Phase 1 work. No change needed.
  </action>
  <verify>
    - `python -c "from gathering.utils.bounded_lru import BoundedLRUDict; d = BoundedLRUDict(max_size=3); d['a']=1; d['b']=2; d['c']=3; d['d']=4; print(list(d.keys())); assert 'a' not in d; print('LRU eviction works')"` passes
    - `python -c "from gathering.agents.memory import MemoryService; print('import ok')"` succeeds
    - `python -c "from gathering.rag.embeddings import EmbeddingService; print('import ok')"` succeeds
    - `python -m pytest tests/ -x -q --timeout=30 2>&1 | tail -20` passes
  </verify>
  <done>BoundedLRUDict utility exists at gathering/utils/bounded_lru.py. All identified unbounded caches (MemoryService._persona_cache, _project_cache, _session_cache; EmbeddingService._memory_cache; AIModelsSkill._embedding_cache) use BoundedLRUDict with configurable max_size. Orchestration EventBus history uses deque(maxlen=N). All caches evict LRU entries when full.</done>
</task>

</tasks>

<verification>
- slowapi installed and configured with per-endpoint tiers
- RateLimitMiddleware no longer added in create_app()
- app.state.limiter exists on created app
- RateLimitExceeded handler returns 429 with Retry-After
- BoundedLRUDict tested for correct eviction behavior
- All identified unbounded caches retrofitted
- All existing tests pass
</verification>

<success_criteria>
1. `pip show slowapi` shows installed version
2. `from gathering.api.rate_limit import limiter` imports cleanly
3. `create_app()` does not reference RateLimitMiddleware
4. `from gathering.utils.bounded_lru import BoundedLRUDict` imports cleanly
5. MemoryService, EmbeddingService, AIModelsSkill caches use BoundedLRUDict
6. `python -m pytest tests/ -x -q --timeout=30` passes
</success_criteria>

<output>
After completion, create `.planning/phases/04-performance-optimization/04-02-SUMMARY.md`
</output>
