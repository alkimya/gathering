---
phase: 03-schedule-execution-tool-hardening
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - gathering/orchestration/scheduler.py
  - gathering/orchestration/pipeline/nodes.py
autonomous: true

must_haves:
  truths:
    - "A schedule with action_type 'execute_pipeline' triggers a real pipeline run via PipelineExecutor"
    - "A schedule with action_type 'send_notification' dispatches via NotificationsSkill"
    - "A schedule with action_type 'call_api' dispatches via HTTPSkill"
    - "After crash/restart, the scheduler detects missed runs whose next_run_at is in the past"
    - "The scheduler does not re-execute actions that already have a completed/running run for the missed window"
    - "Missed runs with no prior run record are executed as recovery runs"
    - "The running_actions set is populated before asyncio.create_task to prevent race conditions"
  artifacts:
    - path: "gathering/orchestration/scheduler.py"
      provides: "Action type dispatcher table and crash recovery logic"
      contains: "ACTION_DISPATCHERS"
    - path: "gathering/orchestration/scheduler.py"
      provides: "_recover_missed_runs method"
      contains: "_recover_missed_runs"
    - path: "gathering/orchestration/pipeline/nodes.py"
      provides: "Real action dispatch for action nodes (replaces Phase 2 stub)"
      contains: "action_dispatchers"
  key_links:
    - from: "gathering/orchestration/scheduler.py"
      to: "gathering/orchestration/pipeline/executor.py"
      via: "PipelineExecutor import and execute() call in _dispatch_execute_pipeline"
      pattern: "PipelineExecutor.*execute"
    - from: "gathering/orchestration/scheduler.py"
      to: "circle.scheduled_action_runs"
      via: "DB query checking for existing runs in missed window"
      pattern: "scheduled_action_runs.*WHERE.*scheduled_action_id"
    - from: "gathering/orchestration/scheduler.py"
      to: "_check_and_execute_due_actions"
      via: "_running_actions.add before create_task"
      pattern: "_running_actions\\.add.*create_task"
---

<objective>
Implement action type dispatchers for the scheduler and crash recovery with deduplication.

Purpose: The scheduler currently only dispatches `run_task` via BackgroundTaskExecutor. This plan adds dispatchers for all four action types (run_task, execute_pipeline, send_notification, call_api) and adds crash recovery logic that detects missed runs on startup without creating duplicates.

Output: Scheduler dispatches real actions, recovers from crashes safely, and pipeline action nodes dispatch real work.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-schedule-execution-tool-hardening/03-RESEARCH.md
@gathering/orchestration/scheduler.py
@gathering/orchestration/pipeline/nodes.py
@gathering/orchestration/pipeline/executor.py
@gathering/skills/gathering/schedules.py
@gathering/db/migrations/001_complete_schema.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add action type dispatchers and reconcile ScheduledAction dataclass</name>
  <files>gathering/orchestration/scheduler.py</files>
  <action>
1. **Reconcile ScheduledAction dataclass with actual DB schema.** The DB (001_complete_schema.sql) uses `action_type VARCHAR(100) NOT NULL` and `action_config JSONB DEFAULT '{}'` columns. The current ScheduledAction dataclass only has `goal` and `agent_id` for task config. Add fields:
   - `action_type: str = "run_task"` (default preserves backward compat)
   - `action_config: Dict[str, Any] = field(default_factory=dict)`
   Update `_row_to_action()` to read `action_type` and `action_config` from the DB row (with fallback defaults for old rows that may only have `goal`).

2. **Create ACTION_DISPATCHERS dispatch table** at module level, mapping action_type strings to async handler functions:
   ```python
   ACTION_DISPATCHERS: Dict[str, Callable] = {
       "run_task": _dispatch_run_task,
       "execute_pipeline": _dispatch_execute_pipeline,
       "send_notification": _dispatch_send_notification,
       "call_api": _dispatch_call_api,
   }
   ```

3. **Implement dispatcher functions** (module-level async functions, NOT methods on Scheduler -- keeps dispatch table clean):

   - `_dispatch_run_task(action, context)`: Extract from existing `_execute_action` logic -- uses BackgroundTaskExecutor + AgentWrapper. This is the existing behavior, just refactored into a dispatcher.

   - `_dispatch_execute_pipeline(action, context)`: Import PipelineExecutor and PipelineRunManager from `gathering.orchestration.pipeline.executor`. Read `pipeline_id` from `action.action_config` (or `action.metadata`). Load pipeline definition (nodes/edges) from DB via `context["db"]`. Create PipelineExecutor instance and call `await executor.execute(run_id=run_id, trigger_data=action.action_config)`. Wrap in try/except NodeExecutionError for clean error reporting.

   - `_dispatch_send_notification(action, context)`: Import and instantiate NotificationsSkill (or get via SkillRegistry). Call `skill.execute("send_notification", action.action_config)`. Log result. If skill not loadable, log warning and return error dict.

   - `_dispatch_call_api(action, context)`: Import and instantiate HTTPSkill (or get via SkillRegistry). Call `skill.execute("http_request", action.action_config)`. Log result. If skill not loadable, log warning and return error dict.

4. **Refactor `_execute_action`** to use the dispatch table:
   - Determine action_type: use `action.action_type` if set, fall back to `"run_task"` for backward compatibility with old rows that only have `goal`.
   - Build context dict: `{"db": self.db_service, "event_bus": self.event_bus}`
   - Look up handler from ACTION_DISPATCHERS. If not found, log error and record failure.
   - Call handler: `result = await handler(action, context)`
   - Keep existing run record creation, event emission, next_run update, and retry logic around the dispatch call.

5. **Fix race condition in `_check_and_execute_due_actions`**: Move `self._running_actions.add(action.id)` INSIDE the lock block, BEFORE `asyncio.create_task(self._execute_action(action))`. Currently, `_running_actions` is only updated inside `_execute_action` after an async context switch, which allows duplicate task creation. The add must happen synchronously before the task is created.

6. **Add `_recover_missed_runs` method** to Scheduler class:
   - Called in `start()` after `_load_actions()` and before creating the run loop task.
   - Iterate over loaded actions. For each ACTIVE action where `next_run_at` is in the past:
     - Query `circle.scheduled_action_runs` for any run with matching `scheduled_action_id` and `triggered_at >= (next_run_at - 60 seconds)` and `status IN ('completed', 'running', 'pending')`.
     - If a run exists: log "Action {id} already has run for missed window, skipping", advance `next_run_at` via `calculate_next_run()`, persist.
     - If no run exists: log "Action {id} missed, executing recovery run", call `await self._execute_action(action, triggered_by="recovery")`.
   - Wrap entire method in try/except to never prevent scheduler startup.

IMPORTANT: All DB queries use parameterized statements (%(param)s syntax) per project convention. No f-string SQL.
  </action>
  <verify>
Run `python -c "from gathering.orchestration.scheduler import Scheduler, ACTION_DISPATCHERS; print('Dispatchers:', list(ACTION_DISPATCHERS.keys()))"` -- should print all 4 action types.
Run `python -c "from gathering.orchestration.scheduler import ScheduledAction; a = ScheduledAction(id=1, agent_id=1, name='test', goal='test', action_type='execute_pipeline', action_config={'pipeline_id': '123'}); print(a.action_type, a.action_config)"` -- should print the action_type and config.
  </verify>
  <done>
Scheduler dispatches all 4 action types via ACTION_DISPATCHERS table. _execute_action uses dispatch table with fallback to run_task. Race condition fixed (running_actions populated before create_task). _recover_missed_runs checks scheduled_action_runs before re-executing missed actions. ScheduledAction dataclass includes action_type and action_config fields.
  </done>
</task>

<task type="auto">
  <name>Task 2: Upgrade pipeline action node to dispatch real actions</name>
  <files>gathering/orchestration/pipeline/nodes.py</files>
  <action>
Replace the Phase 2 stub in `_handle_action` with real action dispatch.

The current stub just logs and returns `{"action": action_type, "executed": True}`. Replace with:

1. Read `action_type` from `node.config.get("action", "unknown")` (existing).
2. Import and reuse the ACTION_DISPATCHERS from scheduler: `from gathering.orchestration.scheduler import ACTION_DISPATCHERS`.
   - HOWEVER, to avoid circular imports, check if this import works. If it causes circular import issues, define a lightweight local dispatch that calls skills directly:
     - For "send_notification": get NotificationsSkill, call execute
     - For "call_api": get HTTPSkill, call execute
     - For "execute_pipeline": raise NodeConfigError("Nested pipeline execution not supported from action nodes")
     - For unknown types: log warning and return metadata dict (graceful degradation, matching Phase 2 pattern)
3. Build context from the `context` dict parameter (which already has db, event_bus from PipelineExecutor).
4. Build a lightweight action-like object or just pass node.config directly to the skill's execute method.
5. If the action type is unknown or dispatch fails with a retryable error, raise NodeExecutionError (which triggers tenacity retry from the executor).
6. If the action type config is invalid, raise NodeConfigError (no retry).
7. Return the result dict from the dispatcher, merged with `{"action": action_type, "executed": True}` for backward compat.

Keep the input summarization logic from the current stub for logging.
  </action>
  <verify>
Run `python -c "from gathering.orchestration.pipeline.nodes import dispatch_node, _handle_action; print('action handler loaded')"` -- should import without errors.
  </verify>
  <done>
Pipeline action nodes dispatch real actions via skill execution instead of returning stub metadata. Unknown action types degrade gracefully. Retryable failures raise NodeExecutionError, config errors raise NodeConfigError.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from gathering.orchestration.scheduler import Scheduler, ACTION_DISPATCHERS; s = Scheduler(); print('OK:', len(ACTION_DISPATCHERS), 'dispatchers')"` -- imports cleanly, 4 dispatchers registered
2. `python -c "from gathering.orchestration.pipeline.nodes import dispatch_node; print('nodes OK')"` -- no import errors
3. `grep -c 'ACTION_DISPATCHERS' gathering/orchestration/scheduler.py` -- dispatch table exists
4. `grep -c '_recover_missed_runs' gathering/orchestration/scheduler.py` -- recovery method exists
5. `grep -c '_running_actions.add' gathering/orchestration/scheduler.py` -- race condition fix present
</verification>

<success_criteria>
- Scheduler._execute_action dispatches to correct handler based on action_type
- All 4 action types have dispatchers: run_task, execute_pipeline, send_notification, call_api
- execute_pipeline dispatcher creates PipelineExecutor and calls execute()
- Crash recovery method queries scheduled_action_runs before re-executing
- Race condition fixed: _running_actions populated before create_task
- Pipeline action nodes dispatch real actions instead of returning stubs
- No circular imports between scheduler and pipeline modules
</success_criteria>

<output>
After completion, create `.planning/phases/03-schedule-execution-tool-hardening/03-01-SUMMARY.md`
</output>
