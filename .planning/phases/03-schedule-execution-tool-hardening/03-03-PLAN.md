---
phase: 03-schedule-execution-tool-hardening
plan: 03
type: execute
wave: 2
depends_on: ["03-01", "03-02"]
files_modified:
  - tests/test_scheduler_recovery.py
  - tests/test_tool_validation.py
autonomous: true

must_haves:
  truths:
    - "Tests prove scheduler dispatches execute_pipeline action type to PipelineExecutor"
    - "Tests prove crash recovery detects missed runs and executes only those without prior run records"
    - "Tests prove crash recovery skips actions that already have a completed run for the missed window"
    - "Tests prove tool validation rejects invalid parameters with descriptive errors"
    - "Tests prove async tools are awaited and sync tools run in executor from async context"
    - "Tests prove workspace path resolution uses WORKSPACE_ROOT over os.getcwd()"
  artifacts:
    - path: "tests/test_scheduler_recovery.py"
      provides: "Scheduler dispatch, crash recovery, and deduplication tests"
      min_lines: 100
    - path: "tests/test_tool_validation.py"
      provides: "Tool validation, async execution, and workspace path tests"
      min_lines: 80
  key_links:
    - from: "tests/test_scheduler_recovery.py"
      to: "gathering/orchestration/scheduler.py"
      via: "imports Scheduler, ScheduledAction, ACTION_DISPATCHERS"
      pattern: "from gathering\\.orchestration\\.scheduler import"
    - from: "tests/test_tool_validation.py"
      to: "gathering/core/tool_registry.py"
      via: "imports ToolRegistry, ToolDefinition"
      pattern: "from gathering\\.core\\.tool_registry import"
    - from: "tests/test_tool_validation.py"
      to: "gathering/api/routers/workspace.py"
      via: "imports or patches get_project_path"
      pattern: "get_project_path"
---

<objective>
Comprehensive tests for scheduler recovery, tool validation, async execution, and workspace path resolution.

Purpose: Proves that action dispatchers work, crash recovery deduplicates correctly, tool parameter validation catches invalid input, async tools execute without blocking, and workspace paths resolve correctly. Covers TEST-05 (scheduler recovery) and validates FEAT-05 through FEAT-08 and RLBL-04.

Output: Two test files covering all Phase 3 success criteria with passing tests.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-schedule-execution-tool-hardening/03-RESEARCH.md
@.planning/phases/03-schedule-execution-tool-hardening/03-01-SUMMARY.md
@.planning/phases/03-schedule-execution-tool-hardening/03-02-SUMMARY.md
@gathering/orchestration/scheduler.py
@gathering/core/tool_registry.py
@gathering/skills/registry.py
@gathering/api/routers/workspace.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Scheduler dispatch and crash recovery tests</name>
  <files>tests/test_scheduler_recovery.py</files>
  <action>
Create `tests/test_scheduler_recovery.py` with pytest + pytest-asyncio tests. Use `pytest.mark.asyncio` decorator (strict mode per Phase 2 convention).

**Test helpers:**
- `make_action(**overrides)` factory returning ScheduledAction with sensible defaults (id=1, agent_id=1, name="test", goal="test goal", schedule_type=ScheduleType.CRON, status=ScheduledActionStatus.ACTIVE, cron_expression="0 * * * *", action_type="run_task", action_config={}).
- `MockDBService` class with `execute`, `execute_one` methods that record calls and return configurable responses. Pattern from Phase 2 tests.

**Tests to write:**

1. **test_action_dispatchers_registered** -- Assert ACTION_DISPATCHERS has keys for all 4 action types: run_task, execute_pipeline, send_notification, call_api.

2. **test_dispatch_execute_pipeline** -- Create a Scheduler with MockDBService. Create an action with action_type="execute_pipeline" and action_config={"pipeline_id": "test-pipe-1"}. Mock PipelineExecutor to capture calls. Call _dispatch_execute_pipeline (or _execute_action) and verify PipelineExecutor was instantiated with correct pipeline_id and execute() was called. Use unittest.mock.patch for PipelineExecutor.

3. **test_dispatch_send_notification** -- Create action with action_type="send_notification" and action_config={"message": "hello", "channel": "email"}. Mock the NotificationsSkill.execute to capture calls. Verify skill.execute was called with the action_config.

4. **test_dispatch_call_api** -- Create action with action_type="call_api" and action_config={"url": "https://example.com", "method": "GET"}. Mock HTTPSkill.execute. Verify call.

5. **test_dispatch_unknown_type_logs_error** -- Create action with action_type="unknown_thing". Call _execute_action. Verify it doesn't crash, logs an error, and the run record gets a failure status.

6. **test_recover_missed_runs_executes_missed** -- Create Scheduler. Load an action with next_run_at in the past (1 hour ago). MockDBService.execute returns empty list for the scheduled_action_runs query (no prior run). Mock _execute_action. Call _recover_missed_runs(). Assert _execute_action was called with triggered_by="recovery".

7. **test_recover_missed_runs_skips_already_completed** -- Same setup but MockDBService returns a row for the runs query (status='completed'). Call _recover_missed_runs(). Assert _execute_action was NOT called. Assert next_run_at was advanced.

8. **test_recover_missed_runs_skips_running** -- Same but status='running'. Assert _execute_action was NOT called.

9. **test_recover_missed_runs_ignores_future_actions** -- Action with next_run_at in the future. Assert _execute_action was NOT called.

10. **test_race_condition_fixed** -- Verify that in _check_and_execute_due_actions, _running_actions is populated before create_task. This is a structural test: mock asyncio.create_task to capture the moment it's called, and verify action.id is already in _running_actions at that point.

11. **test_scheduled_action_dataclass_has_action_type** -- Assert ScheduledAction can be created with action_type and action_config fields.

Use `unittest.mock.AsyncMock`, `unittest.mock.patch`, `unittest.mock.MagicMock` for mocking async methods and external dependencies.
  </action>
  <verify>
Run `python -m pytest tests/test_scheduler_recovery.py -v --tb=short` -- all tests pass.
  </verify>
  <done>
11+ tests covering all 4 action type dispatchers, crash recovery with deduplication (missed runs executed, completed runs skipped), race condition fix verification, and ScheduledAction dataclass update. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Tool validation, async execution, and workspace path tests</name>
  <files>tests/test_tool_validation.py</files>
  <action>
Create `tests/test_tool_validation.py` with tests for FEAT-07, FEAT-08, and RLBL-04.

**Test helpers:**
- `make_tool_def(**overrides)` factory returning ToolDefinition with defaults (name="test_tool", description="Test", category=ToolCategory.UTILITY, function=lambda **kw: kw, required_competencies=[], parameters={...}, returns={}).
- Simple JSON Schema for testing: `{"type": "object", "properties": {"name": {"type": "string"}, "count": {"type": "integer"}}, "required": ["name"]}`.

**ToolRegistry validation tests (FEAT-07):**

1. **test_execute_valid_params_succeeds** -- Register tool with schema requiring string "name" and integer "count". Call execute(name="hello", count=5). Assert it returns the result (not raises).

2. **test_execute_invalid_type_raises** -- Call execute(name=123). Assert ValueError raised with message containing "name" and type info.

3. **test_execute_missing_required_raises** -- Call execute(count=5) (missing "name"). Assert ValueError raised with message mentioning required parameter.

4. **test_execute_extra_params_with_no_additional_properties** -- Schema with `"additionalProperties": false`. Call with extra param. Assert ValueError.

5. **test_execute_no_schema_skips_validation** -- Register tool with parameters={}. Call execute with anything. Assert no validation error.

6. **test_validation_error_includes_path** -- Use nested schema. Call with invalid nested field. Assert error message includes the path to the invalid field.

**ToolRegistry async tests (FEAT-08):**

7. **test_execute_async_awaits_async_function** -- Register tool with async_function=True and an async lambda. Call execute_async(). Assert it returns the result (was properly awaited).

8. **test_execute_async_wraps_sync_in_executor** -- Register tool with async_function=False. Call execute_async(). Assert result returned (sync function ran in executor).

9. **test_execute_sync_rejects_async_tool** -- Register tool with async_function=True. Call execute() (sync). Assert RuntimeError raised telling caller to use execute_async().

10. **test_concurrent_async_tools_run_parallel** -- Register two async tools that each sleep 0.1s. Call both via asyncio.gather on execute_async. Measure total time. Assert total < 0.15s (proves parallel, not sequential 0.2s).

**SkillRegistry validation tests (FEAT-07):**

11. **test_skill_registry_validates_input_schema** -- Create a mock skill class with get_tools_definition returning a tool with input_schema. Register it. Call execute_tool with invalid input. Assert SkillResponse.success is False with validation error.

12. **test_skill_registry_passes_valid_input** -- Same setup with valid input. Assert SkillResponse.success is True.

**Workspace path tests (RLBL-04):**

13. **test_workspace_root_env_var** -- Set WORKSPACE_ROOT=/tmp. Call get_project_path(999). Assert result is /tmp (or /tmp/999 if that dir exists), not os.getcwd().

14. **test_workspace_fallback_to_cwd_with_warning** -- Unset WORKSPACE_ROOT, mock DB to return None. Call get_project_path(999). Assert returns os.getcwd(). Assert logger.warning was called (check with mock).

15. **test_workspace_db_lookup** -- Mock DB to return {"workspace_path": "/projects/myproject"}. Mock os.path.isdir to return True for that path. Call get_project_path(1). Assert returns "/projects/myproject".

Use `unittest.mock.patch`, `os.environ` context managers, `pytest.mark.asyncio` for async tests. Use `monkeypatch` fixture for environment variable manipulation (cleaner than os.environ direct manipulation).
  </action>
  <verify>
Run `python -m pytest tests/test_tool_validation.py -v --tb=short` -- all tests pass.
  </verify>
  <done>
15+ tests covering JSON Schema validation in both ToolRegistry and SkillRegistry, async/sync tool execution paths, concurrent async parallelism, and workspace path resolution with DB/env/fallback. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_scheduler_recovery.py tests/test_tool_validation.py -v --tb=short` -- all tests pass
2. `python -m pytest tests/test_scheduler_recovery.py -v -k "recover"` -- recovery-specific tests pass
3. `python -m pytest tests/test_tool_validation.py -v -k "valid"` -- validation-specific tests pass
4. `python -m pytest tests/ -v --tb=short -x` -- full test suite still passes (no regressions)
</verification>

<success_criteria>
- All scheduler dispatch tests pass (4 action types tested)
- Crash recovery tests prove: missed runs execute, completed runs skip, running runs skip, future actions ignored
- Race condition test verifies _running_actions populated before create_task
- Tool validation tests prove: invalid params rejected, valid params accepted, error messages descriptive
- Async tests prove: async tools awaited, sync tools in executor, concurrent execution is parallel
- Workspace tests prove: WORKSPACE_ROOT used, DB lookup used, cwd fallback only with warning
- Full test suite passes with no regressions
</success_criteria>

<output>
After completion, create `.planning/phases/03-schedule-execution-tool-hardening/03-03-SUMMARY.md`
</output>
